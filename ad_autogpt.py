# -*- coding: utf-8 -*-
"""AD-AUTOGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KUZrsXen3SEFqBV8YMgYnGpCczUa3OIJ
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import google.generativeai as genai
import requests
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import logging
import asyncio
import nest_asyncio
from datetime import datetime

nest_asyncio.apply()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Gemini and spaCy Initialization Functions ---
def setup_gemini(api_key: str):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-pro")

def setup_spacy():
    try:
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        logger.info("Downloading spaCy model...")
        spacy.cli.download("en_core_web_sm")
        nlp = spacy.load("en_core_web_sm")
    return nlp

# --- Web Scraping Function with Selenium ---
async def retrieve_alzheimers_data(driver, main_url):
    driver.get(main_url)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
    time.sleep(3)

    # Get all article links with `rel="bookmark"`
    links = driver.find_elements(By.XPATH, '//a[@rel="bookmark"]')
    articles = []

    for link in links:
        title = link.text
        url = link.get_attribute('href')
        print(f"Scraping: {title} - {url}")
        driver.get(url)
        time.sleep(3)  # Wait for page to load

        page_content = driver.page_source
        soup = BeautifulSoup(page_content, 'html.parser')
        text = soup.get_text(strip=True)  # Retrieve entire page text
        articles.append({'title': title, 'url': url, 'content': text})

    return articles

# --- Analysis Functions ---
def break_into_subprompts(model, main_prompt: str):
    response = model.generate_content(f"Break this task into specific, ordered sub-tasks:\n\n{main_prompt}")
    return [sp.strip() for sp in response.text.split('\n') if sp.strip()]

async def summarize_content(model, text: str):
    response = model.generate_content(f"Provide a concise summary in 3-4 points:\n\n{text}")
    return response.text.strip()

def perform_topic_modeling(texts, num_topics=5):
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
    doc_term_matrix = vectorizer.fit_transform(texts)
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, max_iter=20)
    lda.fit(doc_term_matrix)
    terms = vectorizer.get_feature_names_out()
    return [[terms[i] for i in topic.argsort()[-5:]] for topic in lda.components_]

# --- Main Function for Full Analysis ---
async def analyze_alzheimers_research(api_key, main_prompt):
    model = setup_gemini(api_key)
    nlp = setup_spacy()
    driver = webdriver.Chrome()

    try:
        # Step 1: Retrieve Data
        main_url = 'https://www.nia.nih.gov/health/alzheimers-and-dementia'
        articles = await retrieve_alzheimers_data(driver, main_url)

        # Step 2: Break into Sub-prompts
        sub_prompts = break_into_subprompts(model, main_prompt)

        # Step 3: Process Each Article
        summaries, locations, topics = [], [], []
        for article in articles:
            summary = await summarize_content(model, article['content'])
            summaries.append(summary)

            # Extract topics
            topics += perform_topic_modeling([article['content']])

        driver.quit()  # Close the driver after scraping is complete

        # Compile Results
        return {
            'timestamp': datetime.now().isoformat(),
            'sub_prompts': sub_prompts,
            'summaries': summaries,
            'topics': topics
        }

    except Exception as e:
        logger.error(f"Analysis failed: {str(e)}")
        driver.quit()
        return {'error': str(e)}

# Run the analysis in Jupyter or Script
if __name__ == "__main__":
    api_key = 'YOUR_GEMINI_API_KEY'
    main_prompt = """
    Analyze recent Alzheimer's Disease research:
    1. Retrieve latest research articles
    2. Summarize key findings
    3. Extract geographical information
    4. Identify main research topics
    5. Visualize research locations
    """

    loop = asyncio.get_event_loop()
    results = loop.run_until_complete(analyze_alzheimers_research(api_key, main_prompt))

    # Display Results
    if 'error' not in results:
        print("\nAnalysis Results:")
        for stage, data in results.items():
            print(f"{stage.upper()}:\n{data}")
    else:
        print(f"Analysis failed: {results['error']}")